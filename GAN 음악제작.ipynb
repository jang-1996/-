{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgXVFl5SmZvC"
   },
   "source": [
    "# Generating Music with GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mArAbLs2mBhd"
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeGaPSXNdqy-"
   },
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJ_cVBuhk13r",
    "outputId": "f1c20423-ce07-4d2a-88e8-50cca92d18c3"
   },
   "source": [
    "!pip install torch matplotlib tqdm livelossplot gdown \"pypianoroll>=1.0.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn-74oKmdyF7"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'music21'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmusic21\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m converter, corpus, instrument, midi, note, chord, pitch\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmusic21\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_midi\u001b[39m(midi_path):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# There is an one-line method to read MIDIs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# but to remove the drums we need to manipulate some\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# low level MIDI events.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'music21'"
     ]
    }
   ],
   "source": [
    "from music21 import converter, corpus, instrument, midi, note, chord, pitch\n",
    "from music21 import *\n",
    "def open_midi(midi_path):\n",
    "    # There is an one-line method to read MIDIs\n",
    "    # but to remove the drums we need to manipulate some\n",
    "    # low level MIDI events.\n",
    "    mf = midi.MidiFile()\n",
    "    mf.open(midi_path)\n",
    "    mf.read()\n",
    "    mf.close()\n",
    "\n",
    "    return midi.translate.midiFileToStream(mf)\n",
    "    \n",
    "base_midi = open_midi(r'C:\\Users\\medici\\Documents\\code\\musegan-main\\pytorch\\data\\김광석\\김광석_너무아픈사랑은사랑이아니었음을.mid')\n",
    "base_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting music21\n",
      "  Using cached music21-7.3.1-py3-none-any.whl (22.4 MB)\n",
      "Requirement already satisfied: chardet in c:\\users\\medici\\anaconda3\\lib\\site-packages (from music21) (4.0.0)\n",
      "Collecting webcolors>=1.5\n",
      "  Using cached webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting jsonpickle\n",
      "  Using cached jsonpickle-2.1.0-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\medici\\anaconda3\\lib\\site-packages (from music21) (3.5.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\medici\\anaconda3\\lib\\site-packages (from music21) (1.1.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\medici\\anaconda3\\lib\\site-packages (from music21) (8.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\medici\\anaconda3\\lib\\site-packages (from music21) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\medici\\anaconda3\\lib\\site-packages (from matplotlib->music21) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\medici\\anaconda3\\lib\\site-packages (from matplotlib->music21) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\medici\\anaconda3\\lib\\site-packages (from matplotlib->music21) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\medici\\anaconda3\\lib\\site-packages (from matplotlib->music21) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\medici\\anaconda3\\lib\\site-packages (from matplotlib->music21) (3.0.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\medici\\anaconda3\\lib\\site-packages (from matplotlib->music21) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\medici\\anaconda3\\lib\\site-packages (from matplotlib->music21) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\medici\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->music21) (1.16.0)\n",
      "Installing collected packages: webcolors, jsonpickle, music21\n",
      "Successfully installed jsonpickle-2.1.0 music21-7.3.1 webcolors-1.11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\medici\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\medici\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\medici\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\medici\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\medici\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\medici\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\medici\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\medici\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of instruments found on MIDI file:\n",
      "Flute\n",
      "Fretless Bass\n",
      "Harmonica\n",
      "StringInstrument\n",
      "Electric Guitar\n",
      "Electric Organ\n",
      "StringInstrument\n",
      "Acoustic Guitar\n",
      "Acoustic Guitar\n",
      "Percussion\n",
      "Percussion\n"
     ]
    }
   ],
   "source": [
    "def list_instruments(midi):\n",
    "    partStream = midi.parts.stream()\n",
    "    print(\"List of instruments found on MIDI file:\")\n",
    "    for p in partStream:\n",
    "        aux = p\n",
    "        print (p.partName)\n",
    "\n",
    "list_instruments(base_midi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "zOkT9h38krfZ"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pypianoroll\n",
    "from pypianoroll import Multitrack, Track, BinaryTrack\n",
    "from tqdm import tqdm\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VyXuXFtoLxL"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "PA14sQ-YoTvW"
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "n_tracks = 5  # number of tracks\n",
    "n_pitches = 72  # number of pitches\n",
    "lowest_pitch = 24  # MIDI note number of the lowest pitch\n",
    "n_samples_per_song = 8  # number of samples to extract from each song in the datset\n",
    "n_measures = 4  # number of measures per sample\n",
    "beat_resolution = 4  # temporal resolution of a beat (in timestep)\n",
    "programs = [0, 0, 25, 33, 48]  # program number for each track\n",
    "is_drums = [True, True, True, True, True]  # drum indicator for each track\n",
    "track_names = ['Drums', 'Piano', 'Guitar', 'Bass', 'Strings']  # name of each track\n",
    "tempo = 100\n",
    "\n",
    "# Training\n",
    "batch_size = 16\n",
    "latent_dim = 128\n",
    "n_steps = 100\n",
    "\n",
    "# Sampling\n",
    "sample_interval = 200  # interval to run the sampler (in step)\n",
    "n_samples = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "F91IBYd8mr9f"
   },
   "outputs": [],
   "source": [
    "measure_resolution = 4 * beat_resolution\n",
    "tempo_array = np.full((4 * 4 * measure_resolution, 1), tempo)\n",
    "assert 24 % beat_resolution == 0, (\n",
    "    \"beat_resolution must be a factor of 24 (the beat resolution used in \"\n",
    "    \"the source dataset).\"\n",
    ")\n",
    "assert len(programs) == len(is_drums) and len(programs) == len(track_names), (\n",
    "    \"Lengths of programs, is_drums and track_names must be the same.\"\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JyxQDtXfhbp"
   },
   "source": [
    "## Data Prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79jp0AoJep4X"
   },
   "source": [
    "### Download the Lakh Pianoroll Dataset (LPD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!mkdir result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTdEEg2Xdnbt",
    "outputId": "38bcbe68-99c0-482a-f3ed-2642b84f651b"
   },
   "outputs": [],
   "source": [
    "# !mkdir data\n",
    "\n",
    "# !gdown -O data/lpd_5_cleansed.tar.gz https://drive.google.com/uc?id=1yz0Ma-6cWTl6mhkrLnAVJ7RNzlQRypQ5\n",
    "# !gdown -O data/id_lists_amg.tar.gz https://drive.google.com/uc?id=1hp9b_g1hu_dkP4u8h46iqHeWMaUoI07R\n",
    "# !gdown -O data/id_lists_lastfm.tar.gz https://drive.google.com/uc?id=1mpsoxU2fU1AjKopkcQ8Q8V6wYmVPbnPO\n",
    "\n",
    "# !tar zxf data/lpd_5_cleansed.tar.gz -C data/\n",
    "# !tar zxf data/id_lists_amg.tar.gz -C data/\n",
    "# !tar zxf data/id_lists_lastfm.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoN6wcDDjIDG"
   },
   "source": [
    "dataset_root = Path(\"data/lpd_5/lpd_5_cleansed/\")\n",
    "id_list = []\n",
    "for path in os.listdir(\"data/amg\"):\n",
    "    filepath = os.path.join(\"data/amg\", path)\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath) as f:\n",
    "            id_list.extend([line.rstrip() for line in f])\n",
    "id_list = list(set(id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWDFp1pR5CQd"
   },
   "source": [
    "def msd_id_to_dirs(msd_id):\n",
    "    \"\"\"Given an MSD ID, generate the path prefix.\n",
    "    E.g. TRABCD12345678 -> A/B/C/TRABCD12345678\"\"\"\n",
    "    return os.path.join(msd_id[2], msd_id[3], msd_id[4], msd_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvn3WqbMfaiz"
   },
   "source": [
    "### Visualize an example of pianorolls in LPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "8JPHTDHL54F0",
    "outputId": "d178bb91-37da-4e59-de19-b64d3cc79b0e"
   },
   "source": [
    "song_dir = dataset_root / msd_id_to_dirs('TREVDFX128E07859E0') # 'TRQAOWZ128F93000A4', 'TREVDFX128E07859E0'\n",
    "multitrack = pypianoroll.load(song_dir / os.listdir(song_dir)[0])\n",
    "multitrack.trim(end=12 * 96)\n",
    "axs = multitrack.plot()\n",
    "plt.gcf().set_size_inches((16, 8))\n",
    "for ax in axs:\n",
    "    for x in range(96, 12 * 96, 96):     \n",
    "        ax.axvline(x - 0.5, color='k', linestyle='-', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGRUCRm_CGej",
    "tags": []
   },
   "source": [
    "### Collect training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\김광석\\\\거리에서_김광석.MID',\n",
       " 'data\\\\김광석\\\\그녀가_처음_울던_날.mid',\n",
       " 'data\\\\김광석\\\\김광석-사랑했지만.mid',\n",
       " 'data\\\\김광석\\\\김광석-일어나.mid',\n",
       " 'data\\\\김광석\\\\김광석_너무아픈사랑은사랑이아니었음을.mid',\n",
       " 'data\\\\김광석\\\\김광석_이등병의_편지.mid',\n",
       " 'data\\\\김광석\\\\두바퀴로가는차.mid',\n",
       " 'data\\\\김광석\\\\부치지않은편지.mid',\n",
       " 'data\\\\김광석\\\\서른즈음에.mid']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "many_file_path = \"data\\\\김광석\\\\*.mid\"\n",
    "file_path_list = glob.glob(many_file_path)\n",
    "file_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\김광석\\\\거리에서_김광석.MID'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MPGrsycbhodh",
    "outputId": "fb7c6c0e-d4ca-49fe-dce8-7ad81caf6c99",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully collect 8 samples from 7323 songs\n",
      "Data shape : (8, 5, 64, 72)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "# Iterate over all the songs in the ID list\n",
    "for song_dir in tqdm(file_path_list[0:1]):\n",
    "    # Load the multitrack as a pypianoroll.Multitrack instance\n",
    "    # song_dir = dataset_root / msd_id_to_dirs(msd_id)\n",
    "    multitrack = pypianoroll.read(song_dir)\n",
    "    # Binarize the pianorolls\n",
    "    multitrack.binarize()\n",
    "    # Downsample the pianorolls (shape: n_timesteps x n_pitches)\n",
    "    multitrack.set_resolution(beat_resolution)\n",
    "    # Stack the pianoroll (shape: n_tracks x n_timesteps x n_pitches)\n",
    "    pianoroll = (multitrack.stack() > 0)\n",
    "    # Get the target pitch range only\n",
    "    pianoroll = pianoroll[:, :, lowest_pitch:lowest_pitch + n_pitches]\n",
    "    # Calculate the total measures\n",
    "    n_total_measures = multitrack.get_max_length() // measure_resolution\n",
    "    candidate = n_total_measures - n_measures\n",
    "    target_n_samples = min(n_total_measures // n_measures, n_samples_per_song)\n",
    "    # Randomly select a number of phrases from the multitrack pianoroll\n",
    "    for idx in np.random.choice(candidate, target_n_samples, False):\n",
    "        start = idx * measure_resolution\n",
    "        end = (idx + n_measures) * measure_resolution\n",
    "        # Skip the samples where some track(s) has too few notes\n",
    "        if (pianoroll.sum(axis=(1, 2)) < 10).any():\n",
    "            continue\n",
    "        data.append(pianoroll[:, start:end])\n",
    "# Stack all the collected pianoroll segments into one big array\n",
    "random.shuffle(data)\n",
    "data = np.stack(data)\n",
    "print(f\"Successfully collect {len(data)} samples from {len(id_list)} songs\")\n",
    "print(f\"Data shape : {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26154, 5, 64, 72)\n",
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data/data.npz')['data']\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "print(data[:4][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.savez_compressed('data/data', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqpNtJFae4aF"
   },
   "source": [
    "### Visualize an example of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "3fkoP9rGgrLH",
    "outputId": "45065a63-246c-4031-a625-f47d6ec43eb4"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [189]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m tracks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (program, is_drum, track_name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(programs, is_drums, track_names)):\n\u001b[0;32m      3\u001b[0m     pianoroll \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m----> 4\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, ((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (lowest_pitch, \u001b[38;5;241m128\u001b[39m \u001b[38;5;241m-\u001b[39m lowest_pitch \u001b[38;5;241m-\u001b[39m n_pitches)))\n\u001b[0;32m      5\u001b[0m     tracks\u001b[38;5;241m.\u001b[39mappend(Track(name\u001b[38;5;241m=\u001b[39mtrack_name, program\u001b[38;5;241m=\u001b[39mprogram, is_drum\u001b[38;5;241m=\u001b[39mis_drum, pianoroll\u001b[38;5;241m=\u001b[39mpianoroll))\n\u001b[0;32m      6\u001b[0m multitrack \u001b[38;5;241m=\u001b[39m Multitrack(tracks\u001b[38;5;241m=\u001b[39mtracks, tempo\u001b[38;5;241m=\u001b[39mtempo_array, resolution\u001b[38;5;241m=\u001b[39mbeat_resolution)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "tracks = []\n",
    "for idx, (program, is_drum, track_name) in enumerate(zip(programs, is_drums, track_names)):\n",
    "    pianoroll = np.pad(\n",
    "        np.concatenate(data[:4], 1)[idx], ((0, 0), (lowest_pitch, 128 - lowest_pitch - n_pitches)))\n",
    "    tracks.append(Track(name=track_name, program=program, is_drum=is_drum, pianoroll=pianoroll))\n",
    "multitrack = Multitrack(tracks=tracks, tempo=tempo_array, resolution=beat_resolution)\n",
    "axs = multitrack.plot()\n",
    "plt.gcf().set_size_inches((16, 8))\n",
    "for ax in axs:\n",
    "    for x in range(measure_resolution, 4 * 4 * measure_resolution, measure_resolution):\n",
    "        if x % (measure_resolution * 4) == 0:\n",
    "            ax.axvline(x - 0.5, color='k')\n",
    "        else:\n",
    "            ax.axvline(x - 0.5, color='k', linestyle='-', linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6fauic_H2wt"
   },
   "source": [
    "### Create dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "id": "gBxjQEZFRhAK"
   },
   "outputs": [],
   "source": [
    "data = torch.as_tensor(data, dtype=torch.float32)\n",
    "dataset = torch.utils.data.TensorDataset(data)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, drop_last=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94rrn1nmIQlG"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyX6Duf5fkiw"
   },
   "source": [
    " ### Define the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "Fj9sCKbSKcse"
   },
   "outputs": [],
   "source": [
    "class GeneraterBlock(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
    "        super().__init__()\n",
    "        self.transconv = torch.nn.ConvTranspose3d(in_dim, out_dim, kernel, stride)\n",
    "        self.batchnorm = torch.nn.BatchNorm3d(out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transconv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return torch.nn.functional.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "ZWPAxfkmsIWn"
   },
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    \"\"\"A convolutional neural network (CNN) based generator. The generator takes\n",
    "    as input a latent vector and outputs a fake sample.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transconv0 = GeneraterBlock(latent_dim, 256, (4, 1, 1), (4, 1, 1))\n",
    "        self.transconv1 = GeneraterBlock(256, 128, (1, 4, 1), (1, 4, 1))\n",
    "        self.transconv2 = GeneraterBlock(128, 64, (1, 1, 4), (1, 1, 4))\n",
    "        self.transconv3 = GeneraterBlock(64, 32, (1, 1, 3), (1, 1, 1))\n",
    "        self.transconv4 = torch.nn.ModuleList([\n",
    "            GeneraterBlock(32, 16, (1, 4, 1), (1, 4, 1))\n",
    "            for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.transconv5 = torch.nn.ModuleList([\n",
    "            GeneraterBlock(16, 1, (1, 1, 12), (1, 1, 12))\n",
    "            for _ in range(n_tracks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, latent_dim, 1, 1, 1)\n",
    "        x = self.transconv0(x)\n",
    "        x = self.transconv1(x)\n",
    "        x = self.transconv2(x)\n",
    "        x = self.transconv3(x)\n",
    "        x = [transconv(x) for transconv in self.transconv4]\n",
    "        x = torch.cat([transconv(x_) for x_, transconv in zip(x, self.transconv5)], 1)\n",
    "        x = x.view(-1, n_tracks, n_measures * measure_resolution, n_pitches)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJPGeYNbfvw6"
   },
   "source": [
    " ### Define the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "Knw5u6Px2c8j"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    \"\"\"An implementation of Layer normalization that does not require size\n",
    "    information. Copied from https://github.com/pytorch/pytorch/issues/1959.\"\"\"\n",
    "    def __init__(self, n_features, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.n_features = n_features\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "        if self.affine:\n",
    "            self.gamma = torch.nn.Parameter(torch.Tensor(n_features).uniform_())\n",
    "            self.beta = torch.nn.Parameter(torch.zeros(n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = [-1] + [1] * (x.dim() - 1)\n",
    "        mean = x.view(x.size(0), -1).mean(1).view(*shape)\n",
    "        std = x.view(x.size(0), -1).std(1).view(*shape)\n",
    "        y = (x - mean) / (std + self.eps)\n",
    "        if self.affine:\n",
    "            shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "            y = self.gamma.view(*shape) * y + self.beta.view(*shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "gZhbO2jiFLG5"
   },
   "outputs": [],
   "source": [
    "class DiscriminatorBlock(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, kernel, stride):\n",
    "        super().__init__()\n",
    "        self.transconv = torch.nn.Conv3d(in_dim, out_dim, kernel, stride)\n",
    "        self.layernorm = LayerNorm(out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.transconv(x)\n",
    "        x = self.layernorm(x)\n",
    "        return torch.nn.functional.leaky_relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "kczm8A8Nl78i"
   },
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    \"\"\"A convolutional neural network (CNN) based discriminator. The\n",
    "    discriminator takes as input either a real sample (in the training data) or\n",
    "    a fake sample (generated by the generator) and outputs a scalar indicating\n",
    "    its authentity.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = torch.nn.ModuleList([\n",
    "            DiscriminatorBlock(1, 16, (1, 1, 12), (1, 1, 12)) for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.conv1 = torch.nn.ModuleList([\n",
    "            DiscriminatorBlock(16, 16, (1, 4, 1), (1, 4, 1)) for _ in range(n_tracks)\n",
    "        ])\n",
    "        self.conv2 = DiscriminatorBlock(16 * 5, 64, (1, 1, 3), (1, 1, 1))\n",
    "        self.conv3 = DiscriminatorBlock(64, 64, (1, 1, 4), (1, 1, 4))\n",
    "        self.conv4 = DiscriminatorBlock(64, 128, (1, 4, 1), (1, 4, 1))\n",
    "        self.conv5 = DiscriminatorBlock(128, 128, (2, 1, 1), (1, 1, 1))\n",
    "        self.conv6 = DiscriminatorBlock(128, 256, (3, 1, 1), (3, 1, 1))\n",
    "        self.dense = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, n_tracks, n_measures, measure_resolution, n_pitches)\n",
    "        x = [conv(x[:, [i]]) for i, conv in enumerate(self.conv0)]\n",
    "        x = torch.cat([conv(x_) for x_, conv in zip(x, self.conv1)], 1)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)          \n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiPl8DYCI7pC"
   },
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "5wngyfaaObas"
   },
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    \"\"\"Compute the gradient penalty for regularization. Intuitively, the\n",
    "    gradient penalty help stablize the magnitude of the gradients that the\n",
    "    discriminator provides to the generator, and thus help stablize the training\n",
    "    of the generator.\"\"\"\n",
    "    # Get random interpolations between real and fake samples\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).cuda()\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples))\n",
    "    interpolates = interpolates.requires_grad_(True)\n",
    "    # Get the discriminator output for the interpolations\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    # Get gradients w.r.t. the interpolations\n",
    "    fake = torch.ones(real_samples.size(0), 1).cuda()\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    # Compute gradient penalty\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "x3mgXtVN8ldM"
   },
   "outputs": [],
   "source": [
    "def train_one_step(d_optimizer, g_optimizer, real_samples):\n",
    "    \"\"\"Train the networks for one step.\"\"\"\n",
    "    # Sample from the lantent distribution\n",
    "    latent = torch.randn(batch_size, latent_dim)\n",
    "\n",
    "    # Transfer data to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        real_samples = real_samples.cuda()\n",
    "        latent = latent.cuda()\n",
    "    \n",
    "    # === Train the discriminator ===\n",
    "    # Reset cached gradients to zero\n",
    "    d_optimizer.zero_grad()\n",
    "    # Get discriminator outputs for the real samples\n",
    "    prediction_real = discriminator(real_samples)\n",
    "    # Compute the loss function\n",
    "    # d_loss_real = torch.mean(torch.nn.functional.relu(1. - prediction_real))\n",
    "    d_loss_real = -torch.mean(prediction_real)\n",
    "    # Backpropagate the gradients\n",
    "    d_loss_real.backward()\n",
    "    \n",
    "    # Generate fake samples with the generator\n",
    "    fake_samples = generator(latent)\n",
    "    # Get discriminator outputs for the fake samples\n",
    "    prediction_fake_d = discriminator(fake_samples.detach())\n",
    "    # Compute the loss function\n",
    "    # d_loss_fake = torch.mean(torch.nn.functional.relu(1. + prediction_fake_d))\n",
    "    d_loss_fake = torch.mean(prediction_fake_d)\n",
    "    # Backpropagate the gradients\n",
    "    d_loss_fake.backward()\n",
    "\n",
    "    # Compute gradient penalty\n",
    "    gradient_penalty = 10.0 * compute_gradient_penalty(\n",
    "        discriminator, real_samples.data, fake_samples.data)\n",
    "    # Backpropagate the gradients\n",
    "    gradient_penalty.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    # === Train the generator ===\n",
    "    # Reset cached gradients to zero\n",
    "    g_optimizer.zero_grad()\n",
    "    # Get discriminator outputs for the fake samples\n",
    "    prediction_fake_g = discriminator(fake_samples)\n",
    "    # Compute the loss function\n",
    "    g_loss = -torch.mean(prediction_fake_g)\n",
    "    # Backpropagate the gradients\n",
    "    g_loss.backward()\n",
    "    # Update the weights\n",
    "    g_optimizer.step()\n",
    "\n",
    "    return d_loss_real + d_loss_fake, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ukfh1dxIsDw"
   },
   "source": [
    "## Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCqTBe3p09xY",
    "outputId": "a971f147-6560-46d8-a423-8d5a6cc30b4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in G: 318533\n",
      "Number of parameters in D: 206945\n"
     ]
    }
   ],
   "source": [
    "# Create data loader\n",
    "# data_loader = get_data_loader()\n",
    "\n",
    "# Create neural networks\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "print(\"Number of parameters in G: {}\".format(\n",
    "    sum(p.numel() for p in generator.parameters() if p.requires_grad)))\n",
    "print(\"Number of parameters in D: {}\".format(\n",
    "    sum(p.numel() for p in discriminator.parameters() if p.requires_grad)))\n",
    "\n",
    "# Create optimizers\n",
    "d_optimizer = torch.optim.Adam(\n",
    "    discriminator.parameters(), lr=0.001,  betas=(0.5, 0.9))\n",
    "g_optimizer = torch.optim.Adam(\n",
    "    generator.parameters(), lr=0.001, betas=(0.5, 0.9))\n",
    "\n",
    "# Prepare the inputs for the sampler, which wil run during the training\n",
    "sample_latent = torch.randn(n_samples, latent_dim)\n",
    "\n",
    "# Transfer the neural nets and samples to GPU\n",
    "if torch.cuda.is_available():\n",
    "    discriminator = discriminator.cuda()\n",
    "    generator = generator.cuda()\n",
    "    sample_latent = sample_latent.cuda()\n",
    "\n",
    "# Create an empty dictionary to sotre history samples\n",
    "history_samples = {}\n",
    "\n",
    "# Create a LiveLoss logger instance for monitoring\n",
    "liveloss = PlotLosses(outputs=[MatplotlibPlot(cell_size=(6,2))])\n",
    "\n",
    "# Initialize step\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL499fTNJcSd",
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "JsCO34_A3N2U",
    "outputId": "626c0504-1491-4039-9e26-82f76f331c98",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                   | 0/100 [02:40<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [203]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Start iterations\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step \u001b[38;5;241m<\u001b[39m n_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      6\u001b[0m     \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Iterate over the dataset\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m real_samples \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m      9\u001b[0m         \n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Train the neural networks\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         generator\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     13\u001b[0m         d_loss, g_loss \u001b[38;5;241m=\u001b[39m train_one_step(d_optimizer, g_optimizer, real_samples[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:569\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\utils\\data\\sampler.py:226\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[List[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    225\u001b[0m     batch \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[0;32m    227\u001b[0m         batch\u001b[38;5;241m.\u001b[39mappend(idx)\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\utils\\data\\sampler.py:110\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_source)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    111\u001b[0m     generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mGenerator()\n\u001b[0;32m    112\u001b[0m     generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a progress bar instance for monitoring\n",
    "progress_bar = tqdm(total=n_steps, initial=step, ncols=80, mininterval=1)\n",
    "\n",
    "# Start iterations\n",
    "while step < n_steps + 1:\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for real_samples in data_loader:\n",
    "        \n",
    "\n",
    "        # Train the neural networks\n",
    "        generator.train()\n",
    "        d_loss, g_loss = train_one_step(d_optimizer, g_optimizer, real_samples[0])\n",
    "\n",
    "#         # Record smoothened loss values to LiveLoss logger\n",
    "#         if step > 0:\n",
    "#             running_d_loss = 0.05 * d_loss + 0.95 * running_d_loss\n",
    "#             running_g_loss = 0.05 * g_loss + 0.95 * running_g_loss\n",
    "#         else:\n",
    "#             running_d_loss, running_g_loss = 0.0, 0.0\n",
    "#         liveloss.update({'negative_critic_loss': -running_d_loss})\n",
    "#         # liveloss.update({'d_loss': running_d_loss, 'g_loss': running_g_loss})\n",
    "\n",
    "#         # Update losses to progress bar\n",
    "#         progress_bar.set_description_str(\n",
    "#             \"(d_loss={: 8.6f}, g_loss={: 8.6f})\".format(d_loss, g_loss))\n",
    "\n",
    "        if step % sample_interval == 0:\n",
    "            # Get generated samples\n",
    "            generator.eval()\n",
    "            samples = generator(sample_latent).cpu().detach().numpy()\n",
    "            history_samples[step] = samples\n",
    "\n",
    "            # # Display loss curves\n",
    "            # clear_output(True)\n",
    "            # if step > 0:\n",
    "            #     liveloss.send()\n",
    "\n",
    "            # Display generated samples\n",
    "            samples = samples.transpose(1, 0, 2, 3).reshape(n_tracks, -1, n_pitches)\n",
    "            tracks = []\n",
    "            for idx, (program, is_drum, track_name) in enumerate(\n",
    "                zip(programs, is_drums, track_names)\n",
    "            ):\n",
    "                pianoroll = np.pad(\n",
    "                    samples[idx] > 0.5,\n",
    "                    ((0, 0), (lowest_pitch, 128 - lowest_pitch - n_pitches))\n",
    "                )\n",
    "                tracks.append(\n",
    "                    BinaryTrack(\n",
    "                        name=track_name,\n",
    "                        program=program,\n",
    "                        is_drum=is_drum,\n",
    "                        pianoroll=pianoroll\n",
    "                    )\n",
    "                )\n",
    "            m = Multitrack(\n",
    "                tracks=tracks,\n",
    "                tempo=tempo_array,\n",
    "                resolution=beat_resolution\n",
    "            )\n",
    "\n",
    "            m.write(f'result/{str(step).zfill(5)}.mid')\n",
    "\n",
    "\n",
    "#             axs = m.plot()\n",
    "#             plt.gcf().set_size_inches((16, 8))\n",
    "#             for ax in axs:\n",
    "#                 for x in range(\n",
    "#                     measure_resolution,\n",
    "#                     4 * measure_resolution * n_measures,\n",
    "#                     measure_resolution\n",
    "#                 ):\n",
    "#                     if x % (measure_resolution * 4) == 0:\n",
    "#                         ax.axvline(x - 0.5, color='k')\n",
    "#                     else:\n",
    "#                         ax.axvline(x - 0.5, color='k', linestyle='-', linewidth=1)\n",
    "#             plt.show()\n",
    "\n",
    "        step += 1\n",
    "        progress_bar.update(1)\n",
    "        if step >= n_steps:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true,
    "id": "aeJpOeUXjdBu",
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 5, 64, 72]' is invalid for input of size 129024",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [206]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     generator \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m      9\u001b[0m     sample_latent \u001b[38;5;241m=\u001b[39m sample_latent\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m---> 11\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_latent\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Display generated samples\u001b[39;00m\n\u001b[0;32m     14\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(n_tracks, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_pitches)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GPU\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [192]\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m [transconv(x) \u001b[38;5;28;01mfor\u001b[39;00m transconv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransconv4]\n\u001b[0;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([transconv(x_) \u001b[38;5;28;01mfor\u001b[39;00m x_, transconv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransconv5)], \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tracks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_measures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmeasure_resolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_pitches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 5, 64, 72]' is invalid for input of size 129024"
     ]
    }
   ],
   "source": [
    "generator.eval()\n",
    "\n",
    "sample_latent = torch.randn(n_samples, latent_dim)\n",
    "\n",
    "# Transfer the neural nets and samples to GPU\n",
    "if torch.cuda.is_available():\n",
    "    discriminator = discriminator.cuda()\n",
    "    generator = generator.cuda()\n",
    "    sample_latent = sample_latent.cuda()\n",
    "\n",
    "samples = generator(sample_latent).cpu().detach().numpy()\n",
    "\n",
    "# Display generated samples\n",
    "samples = samples.transpose(1, 0, 2, 3).reshape(n_tracks, -1, n_pitches)\n",
    "tracks = []\n",
    "for idx, (program, is_drum, track_name) in enumerate(\n",
    "    zip(programs, is_drums, track_names)\n",
    "):\n",
    "    pianoroll = np.pad(\n",
    "        samples[idx] > 0.5,\n",
    "        ((0, 0), (lowest_pitch, 128 - lowest_pitch - n_pitches))\n",
    "    )\n",
    "    tracks.append(\n",
    "        BinaryTrack(\n",
    "            name=track_name,\n",
    "            program=program,\n",
    "            is_drum=is_drum,\n",
    "            pianoroll=pianoroll\n",
    "        )\n",
    "    )\n",
    "m = Multitrack(\n",
    "    tracks=tracks,\n",
    "    tempo=tempo_array,\n",
    "    resolution=beat_resolution\n",
    ")\n",
    "axs = m.plot()\n",
    "plt.gcf().set_size_inches((16, 8))\n",
    "for ax in axs:\n",
    "    for x in range(\n",
    "        measure_resolution,\n",
    "        4 * measure_resolution * n_measures,\n",
    "        measure_resolution\n",
    "    ):\n",
    "        if x % (measure_resolution * 4) == 0:\n",
    "            ax.axvline(x - 0.5, color='k')\n",
    "        else:\n",
    "            ax.axvline(x - 0.5, color='k', linestyle='-', linewidth=1)\n",
    "plt.show()\n",
    "\n",
    "m.write('20000.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Music Generation with GANs.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
